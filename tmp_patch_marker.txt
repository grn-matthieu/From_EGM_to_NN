module NNTrain

using Lux
using Zygote
using Optimisers
using Statistics
using Dates
using Printf
using Random
using ..NNData: grid_minibatches
using ..NNLoss: anneal_lambda

using ..NNInit: NNState, init_state

export train!, dummy_epoch!

# Default λ scheduling parameters (overridable via cfg)
const DEFAULT_Λ_START = 0.1
const DEFAULT_Λ_FINAL = 5.0
const DEFAULT_Λ_SCHEDULE = :cosine

"""
    curriculum(epoch, E; stages=default_stages()) -> NamedTuple

Coarse→fine schedule over epochs. Selects an active stage from `stages`
based on the current `epoch` in 1..E and returns its parameters as a
NamedTuple.

Fields in the returned NamedTuple:
- `grid_stride`: Subsample factor for state grid or minibatch indices.
- `nMC`: Number of Monte Carlo draws for expectations.
- `shock_noise`: Multiplier applied to innovation std in the sampler.
"""
function curriculum(epoch::Integer, E::Integer; stages::Vector = default_stages())
    E <= 0 && throw(ArgumentError("E must be positive, got $(E)"))
    epoch < 1 && throw(ArgumentError("epoch must be ≥ 1, got $(epoch)"))
    nstages = length(stages)
    nstages == 0 && throw(ArgumentError("stages must be non-empty"))
    # Map epoch ∈ [1, E] to stage index ∈ [1, nstages]
    frac = clamp(epoch / E, 0.0, 1.0)
    idx = max(1, min(nstages, ceil(Int, frac * nstages)))
    st = stages[idx]
    return (
        name = get(st, :name, Symbol("stage$(idx)")),
        grid_stride = get(st, :grid_stride, 1),
        nMC = get(st, :nMC, 1),
        shock_noise = get(st, :shock_noise, 1.0),
    )
end

"""
    default_stages() -> Vector{NamedTuple}

Default curriculum schedule (override via config):
[
  (; name=:warmup, grid_stride=4, nMC=1, shock_noise=1.25),
  (; name=:mid,    grid_stride=2, nMC=2, shock_noise=1.00),
  (; name=:fine,   grid_stride=1, nMC=4, shock_noise=0.75),
]
"""
default_stages() = [
    (; name = :warmup, grid_stride = 4, nMC = 1, shock_noise = 1.25),
    (; name = :mid, grid_stride = 2, nMC = 2, shock_noise = 1.00),
    (; name = :fine, grid_stride = 1, nMC = 4, shock_noise = 0.75),
]

"""Thin a batch-like array by taking every `stride`-th sample.

If `x` is 2D (features × samples), subsamples columns. If 1D, subsamples
elements. Non-arrays or `stride ≤ 1` are returned unchanged."""
function _thin(x, stride::Integer)
    stride ≤ 1 && return x
    if x isa AbstractArray
        if ndims(x) == 1
            return view(x, 1:stride:length(x))
        elseif ndims(x) ≥ 2
            ns = size(x, 2)
            return @views x[:, 1:stride:ns]
        end
    end
    return x
end

"""
    CSVLogger(path::AbstractString)

Lightweight CSV logger that appends rows. Creates parent directories and
auto-writes a header if the file does not exist or is empty.
"""
struct CSVLogger
    path::String
    header_written::Base.RefValue{Bool}
end

function CSVLogger(path::AbstractString)
    p = String(path)
    mkpath(dirname(p))
    header_written = Base.RefValue(false)
    if isfile(p) && filesize(p) > 0
        header_written[] = true
    end
    return CSVLogger(p, header_written)
end

function log_row!(
    lg::CSVLogger;
    epoch::Integer,
    step::Integer,
    split::AbstractString,
    loss::Real,
    grad_norm::Real,
    lr::Real,
    # Optional curriculum logging fields (per-epoch)
    stage = nothing,
    grid_stride = nothing,
    nMC = nothing,
    shock_noise = nothing,
    lambda_penalty = nothing,
)
    open(lg.path, lg.header_written[] ? "a" : "w") do io
        if !lg.header_written[]
            println(io, "timestamp,epoch,step,split,loss,grad_norm,lr,stage,grid_stride,nMC,shock_noise,lambda_penalty")
            lg.header_written[] = true
        end
        ts = Dates.format(Dates.now(), dateformat"yyyy-mm-ddTHH:MM:SS")
        # Helper to print optional values as NA when absent
        _s(x) = x === nothing ? "NA" : string(x)
        _i(x) = x === nothing ? "NA" : string(Int(x))
        _f(x) = x === nothing ? "NA" : @sprintf("%.6e", float(x))
        # Write full row including curriculum columns
        @printf(io, "%s,%d,%d,%s,%.6e,%.6e,%.6e,%s,%s,%s,%s,%s\n",
            ts,
            epoch,
            step,
            split,
            float(loss),
            float(grad_norm),
            float(lr),
            _s(stage),
            _i(grid_stride),
            _i(nMC),
            _f(shock_noise),
            _f(lambda_penalty),
        )
        lam = I�
    end
    return nothing
end


"""
    EarlyStopping(; patience=5, min_delta=0.0)

Tracks the best metric and signals when to stop.
"""
Base.@kwdef mutable struct EarlyStopping
    patience::Int = 5
    min_delta::Float64 = 0.0
    best::Float64 = Inf
    num_bad::Int = 0
    enabled::Bool = true
end

function reset!(es::EarlyStopping)
    es.best = Inf
    es.num_bad = 0
    return es
end

"""
    should_stop!(es, metric) -> Bool

Updates internal counters and returns true if early stop is triggered.
Lower metric is considered better.
"""
function should_stop!(es::EarlyStopping, metric::Real)
    if !es.enabled
        return false
    end
    if metric < es.best - es.min_delta
        es.best = float(metric)
        es.num_bad = 0
        return false
    else
        es.num_bad += 1
        return es.num_bad >= es.patience
    end
end


# ---- Small utilities over parameter trees ----

"""Apply function `f(::AbstractArray)` to each array leaf in a nested tree."""
function foreach_array_leaf(x, f::F) where {F}
    if x isa NamedTuple
        for v in values(x)
            foreach_array_leaf(v, f)
        end
    elseif x isa Tuple
        for v in x
            foreach_array_leaf(v, f)
        end
    elseif x isa AbstractArray
        f(x)
    elseif x === nothing
        return
    else
        # numbers or other leaves are ignored
        return
    end
end

"""Compute global L2 norm of a gradient tree (sum of leaf Frobenius norms)."""
function grad_global_l2norm(grads)::Float64
    s = 0.0
    foreach_array_leaf(grads) do g
        s += sum(abs2, g)
    end
    return sqrt(s)
end

"""Scale all array leaves by factor `α` in place."""
function scale_grads!(grads, α::Real)
    foreach_array_leaf(grads) do g
        @. g = α * g
    end
    return grads
end


# ---- Core training step ----

"""
    _loss_and_state(model, ps, st, x, y)

Forward pass returning MSE loss and the next state. Shapes are flexible; as a
convention `x` and `y` should be arrays whose leading dimension indexes
features, consistent with Lux.
"""
function _loss_and_state(model, ps, st, x, y)
    ŷ, st2 = Lux.apply(model, x, ps, st)
    return mean(abs2, ŷ .- y), st2
end

"""
    _step!(state::NNState, x, y; clip_norm=nothing)

Performs a single optimisation step and returns `(new_state, loss, grad_norm, lr)`.
`NNState` is immutable, so a new updated instance is returned.
"""
function _step!(
    state::NNState,
    x,
    y;
    clip_norm = nothing,
    nMC::Integer = 1,
    shock_noise::Real = 1.0,
    λ::Real = NaN,
)
    # Compute loss and grads (grads w.r.t. parameters only)
    # We compute grads against the current state.st; a fresh state from the
    # forward pass is then stored for the next iteration.
    loss_val, st_new = _loss_and_state(state.model, state.ps, state.st, x, y)
    gs = first(
        Zygote.gradient(
            p -> first(_loss_and_state(state.model, p, state.st, x, y)),
            state.ps,
        ),
    )

    gnorm = grad_global_l2norm(gs)
    if clip_norm !== nothing && isfinite(clip_norm) && clip_norm > 0 && gnorm > clip_norm
        scale_grads!(gs, clip_norm / (gnorm + 1e-12))
    end

    # Optimiser update (produces new optstate and parameters)
    new_optstate, new_ps = Optimisers.update(state.optstate, state.ps, gs)

    # Learning rate (best-effort; may be absent in some rules)
    lr = try
        getfield(state.opt, :eta)
    catch
        try
            getfield(state.opt, :lr)
        catch
            NaN
        end
    end

    # Note: `nMC` and `shock_noise` are provided for integration with
    # Euler-residual/sampler-based objectives. They are not used in this
    # generic MSE example but are threaded through the training loop.

    new_state = NNState(
        model = state.model,
        ps = new_ps,
        st = st_new,
        opt = state.opt,
        optstate = new_optstate,
        rngs = state.rngs,
    )
    return (new_state, float(loss_val), float(gnorm), float(lr))
end


# ---- Public API ----

"""
    train!(state::NNState, data, cfg; val_data=nothing)

Run a simple epoch-based training loop with optional gradient clipping,
early stopping, and CSV logging.

Configuration keys (symbol-keyed dictionaries expected):
  - `cfg[:solver][:epochs]` (Int, default 1)
  - `cfg[:solver][:clip_norm]` (Real or nothing)
  - `cfg[:solver][:patience]` (Int, default 5)
  - `cfg[:solver][:min_delta]` (Real, default 0.0)
  - `cfg[:logging][:dir]` (String, default "logs")
"""
function train!(state::NNState, data, cfg; val_data = nothing)
    solver_cfg = get(cfg, :solver, Dict{Symbol,Any}())
    log_cfg = get(cfg, :logging, Dict{Symbol,Any}())

    epochs = get(solver_cfg, :epochs, 1)
    clip_norm = get(solver_cfg, :clip_norm, nothing)
    # Curriculum stages: allow override via cfg[:solver][:stages]
    stages_cfg = get(solver_cfg, :stages, default_stages())
    es = EarlyStopping(
        patience = get(solver_cfg, :patience, 5),
        min_delta = float(get(solver_cfg, :min_delta, 0.0)),
        enabled = get(solver_cfg, :early_stopping, true),
    )

    log_dir = get(log_cfg, :dir, joinpath(pwd(), "logs"))
    logger = CSVLogger(joinpath(String(log_dir), "train.csv"))

    for epoch = 1:epochs
        st = curriculum(epoch, epochs; stages = stages_cfg)
        # Per-epoch constraint weight λ (for penalty scheduling)
        λ = anneal_lambda(
            epoch,
            epochs;
            λ_start = get(solver_cfg, :lambda_start, DEFAULT_Λ_START),
            λ_final = get(solver_cfg, :lambda_final, DEFAULT_Λ_FINAL),
            schedule = get(solver_cfg, :lambda_schedule, DEFAULT_Λ_SCHEDULE),
        )
        step = 0
        loss_accum = 0.0
        n_steps = 0
        for (x, y) in data
            # Apply coarse→fine thinning on the batch
            xI, yI = _thin(x, st.grid_stride), _thin(y, st.grid_stride)
            step += 1
            n_steps += 1
            state, loss, gnorm, lr = _step!(
                state,
                xI,
                yI;
                clip_norm = clip_norm,
                nMC = st.nMC,
                shock_noise = st.shock_noise,
                λ = λ,
            )
            loss_accum += loss
            log_row!(logger; epoch, step, split = "train", loss, grad_norm = gnorm, lr)
        end
        train_epoch_loss = n_steps > 0 ? loss_accum / n_steps : NaN

        # Per-epoch summary log row with curriculum fields
        log_row!(
            logger;
            epoch,
            step = 0,
            split = "epoch",
            loss = train_epoch_loss,
            grad_norm = NaN,
            lr = NaN,
            stage = st.name,
            grid_stride = st.grid_stride,
            nMC = st.nMC,
            shock_noise = st.shock_noise,
            lambda_penalty = I�,
        )

        # Stdout one-line summary per epoch
        @info "epoch=$(epoch), loss=$(round(train_epoch_loss, sigdigits=4)), stage=$(st.name), stride=$(st.grid_stride), nMC=$(st.nMC), noise=$(st.shock_noise), λ=$(round(I�, sigdigits=3))"

        # Validation pass (no updates)
        val_epoch_loss = NaN
        if val_data !== nothing
            vloss_accum = 0.0
            v_steps = 0
            for (xv, yv) in val_data
                v_steps += 1
                l, _ = _loss_and_state(state.model, state.ps, state.st, xv, yv)
                vloss_accum += l
            end
            val_epoch_loss = v_steps > 0 ? vloss_accum / v_steps : NaN
            log_row!(
                logger;
                epoch,
                step = 0,
                split = "val",
                loss = val_epoch_loss,
                grad_norm = 0.0,
                lr = NaN,
            )
        end

        metric = isfinite(val_epoch_loss) ? val_epoch_loss : train_epoch_loss
        if should_stop!(es, metric)
            break
        end
    end

    return state
end


"""
    train!(model, data, cfg)

Convenience overload that accepts either an `NNState` or a Lux model. If a Lux
model is passed, a new `NNState` is initialised from `cfg` using
`NNInit.init_state(cfg)`.
"""
function train!(model, data, cfg)
    if model isa NNState
        return train!(model::NNState, data, cfg)
    else
        # Fall back to a default state from configuration.
        state = init_state(cfg)
        return train!(state, data, cfg)
    end
end


# ---- Dummy epoch helper for quick smoke test ----

"""
    dummy_epoch!(; n=64, batch=16, epochs=1, seed=123)

Runs a single small training session on synthetic data so that the training
loop can be quickly smoke-tested. Returns the final `NNState`.
"""
function dummy_epoch!(; n::Int = 64, batch::Int = 16, epochs::Int = 1, seed::Int = 123)
    rng = Random.MersenneTwister(seed)
    # Simple 1D function approximation: y = sin(x)
    x = reshape(range(0, 2π; length = n) .+ 0.05 .* randn(rng, n), 1, :)
    y = reshape(sin.(x), 1, :)

    function make_loader(X, Y, bs)
        idxs = collect(1:size(X, 2))
        Iterators.map(1:ceil(Int, length(idxs) / bs)) do i
            s = (i - 1) * bs + 1
            e = min(i * bs, length(idxs))
            cols = idxs[s:e]
            (X[:, cols], Y[:, cols])
        end
    end

    cfg = Dict{Symbol,Any}(
        :solver => Dict{Symbol,Any}(:epochs => epochs, :hidden => (16, 16)),
        :logging => Dict{Symbol,Any}(:dir => joinpath(pwd(), "logs")),
    )
    state = init_state(cfg)
    train_loader = make_loader(x, y, batch)
    return train!(state, train_loader, cfg)
end

end # module

