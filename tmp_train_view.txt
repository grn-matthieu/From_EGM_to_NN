module NNTrain

using Lux
using Zygote
using Optimisers
include("optim.jl")
using .NNOptim
using Statistics
using Dates
using Printf
using Random
using ..NNData: grid_minibatches
using ..NNLoss: anneal_lambda

using ..NNInit: NNState, init_state

export train!, dummy_epoch!

## Default lambda scheduling parameters (overridable via cfg)
const DEFAULT_LAMBDA_START = 0.1
const DEFAULT_LAMBDA_FINAL = 5.0
const DEFAULT_LAMBDA_SCHEDULE = :cosine

"""
    curriculum(epoch, E; stages=default_stages()) -> NamedTuple

Coarse→fine schedule over epochs. Selects an active stage from `stages`
based on the current `epoch` in 1..E and returns its parameters as a
NamedTuple.

Fields in the returned NamedTuple:
- `grid_stride`: Subsample factor for state grid or minibatch indices.
- `nMC`: Number of Monte Carlo draws for expectations.
- `shock_noise`: Multiplier applied to innovation std in the sampler.
"""
function curriculum(epoch::Integer, E::Integer; stages::Vector = default_stages())
    E <= 0 && throw(ArgumentError("E must be positive, got $(E)"))
    epoch < 1 && throw(ArgumentError("epoch must be >= 1, got $(epoch)"))
    nstages = length(stages)
    nstages == 0 && throw(ArgumentError("stages must be non-empty"))
    # Map epoch ∈ [1, E] to stage index ∈ [1, nstages]
    frac = clamp(epoch / E, 0.0, 1.0)
    idx = max(1, min(nstages, ceil(Int, frac * nstages)))
    st = stages[idx]
    return (
        name = get(st, :name, Symbol("stage$(idx)")),
        grid_stride = get(st, :grid_stride, 1),
        nMC = get(st, :nMC, 1),
        shock_noise = get(st, :shock_noise, 1.0),
    )
end

"""
    default_stages() -> Vector{NamedTuple}

Default curriculum schedule (override via config):
[
  (; name=:warmup, grid_stride=4, nMC=1, shock_noise=1.25),
  (; name=:mid,    grid_stride=2, nMC=2, shock_noise=1.00),
  (; name=:fine,   grid_stride=1, nMC=4, shock_noise=0.75),
]
"""
default_stages() = [
    (; name = :warmup, grid_stride = 4, nMC = 1, shock_noise = 1.25),
    (; name = :mid, grid_stride = 2, nMC = 2, shock_noise = 1.00),
    (; name = :fine, grid_stride <= 1, nMC = 4, shock_noise = 0.75),
]

"""Thin a batch-like array by taking every `stride`-th sample.

If `x` is 2D (features × samples), subsamples columns. If 1D, subsamples
elements. Non-arrays or `stride <= 1` are returned unchanged."""
function _thin(x, stride::Integer)
    stride <= 1 && return x
    if x isa AbstractArray
        if ndims(x) == 1
            return view(x, 1:stride:length(x))
        elseif ndims(x) >= 2
            ns = size(x, 2)
            return @views x[:, 1:stride:ns]
        end
    end
    return x
end

"""
    CSVLogger(path::AbstractString)

Lightweight CSV logger that appends rows. Creates parent directories and
auto-writes a header if the file does not exist or is empty.
"""
struct CSVLogger
    path::String
    header_written::Base.RefValue{Bool}
end

function CSVLogger(path::AbstractString)
    p = String(path)
    mkpath(dirname(p))
    header_written = Base.RefValue(false)
    if isfile(p) && filesize(p) > 0
        header_written[] = true
    end
    return CSVLogger(p, header_written)
end

function log_row!(
    lg::CSVLogger;
    epoch::Integer,
    step::Integer,
    split::AbstractString,
    loss::Real,
    grad_norm::Real,
    lr::Real,
    # Optional curriculum logging fields (per-epoch)
    stage = nothing,
    grid_stride = nothing,
    nMC = nothing,
    shock_noise = nothing,
    lambda_penalty = nothing,
)
    open(lg.path, lg.header_written[] ? "a" : "w") do io
        if !lg.header_written[]
            println(
                io,
                "timestamp,epoch,step,split,loss,grad_norm,lr,stage,grid_stride,nMC,shock_noise,lambda_penalty",
            )
            lg.header_written[] = true
        end
        ts = Dates.format(Dates.now(), dateformat"yyyy-mm-ddTHH:MM:SS")
        # Helper to print optional values as NA when absent
        _s(x) = x === nothing ? "NA" : string(x)
        _i(x) = x === nothing ? "NA" : string(Int(x))
        _f(x) = x === nothing ? "NA" : @sprintf("%.6e", float(x))
        # Write full row including curriculum columns
        @printf(
            io,
            "%s,%d,%d,%s,%.6e,%.6e,%.6e,%s,%s,%s,%s,%s\n",
            ts,
            epoch,
            step,
            split,
            float(loss),
            float(grad_norm),
            float(lr),
            _s(stage),
            _i(grid_stride),
            _i(nMC),
            _f(shock_noise),
            _f(lambda_penalty),
        )
        λ
    end
    return nothing
end


"""
    EarlyStopping(; patience=5, min_delta=0.0)

Tracks the best metric and signals when to stop.
"""
Base.@kwdef mutable struct EarlyStopping
    patience::Int = 5
    min_delta::Float64 = 0.0
    best::Float64 = Inf
    num_bad::Int = 0
    enabled::Bool = true
end

function reset!(es::EarlyStopping)
    es.best = Inf
    es.num_bad = 0
    return es
end

"""
    should_stop!(es, metric) -> Bool

Updates internal counters and returns true if early stop is triggered.
Lower metric is considered better.
"""
function should_stop!(es::EarlyStopping, metric::Real)
    if !es.enabled
        return false
    end
    if metric < es.best - es.min_delta
        es.best = float(metric)
        es.num_bad = 0
        return false
    else
        es.num_bad += 1
        return es.num_bad >= es.patience
    end
end


# ---- Small utilities over parameter trees ----

"""Apply function `f(::AbstractArray)` to each array leaf in a nested tree."""
function foreach_array_leaf(x, f::F) where {F}
    if x isa NamedTuple
        for v in values(x)
            foreach_array_leaf(v, f)
        end
    elseif x isa Tuple
        for v in x
            foreach_array_leaf(v, f)
        end
    elseif x isa AbstractArray
        f(x)
    elseif x === nothing
        return
    else
        # numbers or other leaves are ignored
        return
    end
end

"""Compute global L2 norm of a gradient tree (sum of leaf Frobenius norms)."""
function grad_global_l2norm(grads)::Float64
    s = 0.0
    foreach_array_leaf(grads) do g
        s += sum(abs2, g)
    end
    return sqrt(s)
end

"""Scale all array leaves by factor `α` in place."""
function scale_grads!(grads, α::Real)
    foreach_array_leaf(grads) do g
        @. g = α * g
    end
    return grads
end


# ---- Core training step ----

"""
    _loss_and_state(model, ps, st, x, y)

Forward pass returning MSE loss and the next state. Shapes are flexible; as a
convention `x` and `y` should be arrays whose leading dimension indexes
features, consistent with Lux.
"""
function _loss_and_state(model, ps, st, x, y)
    ŷ, st2 = Lux.apply(model, x, ps, st)
    return mean(abs2, ŷ .- y), st2
end

# Collect array leaves utility for params/grads vectors
collect_array_leaves(x) = begin
    acc = Vector{AbstractArray}()
    foreach_array_leaf(x) do a
        push!(acc, a)
    end
    acc
end

"""
    _step!(state::NNState, x, y; clip_norm=nothing)

Performs a single optimisation step and returns `(new_state, loss, grad_norm, lr)`.
`NNState` is immutable, so a new updated instance is returned.
"""
function _step!(
    state::NNState,
    x,
    y;
    clip_norm = nothing,
    nMC::Integer = 1,
    shock_noise::Real = 1.0,
    λ::Real = NaN,
)
    # Compute loss and grads (grads w.r.t. parameters only)
    # We compute grads against the current state.st; a fresh state from the
    # forward pass is then stored for the next iteration.
    loss_val, st_new = _loss_and_state(state.model, state.ps, state.st, x, y)
    gs = first(
        Zygote.gradient(
            p -> first(_loss_and_state(state.model, p, state.st, x, y)),
            state.ps,
        ),
    )

    gnorm = grad_global_l2norm(gs)
    if clip_norm !== nothing && isfinite(clip_norm) && clip_norm > 0 && gnorm > clip_norm
        scale_grads!(gs, clip_norm / (gnorm + 1e-12))
    end

    # Optimiser update (prefer NNOptim if available)
    new_ps = state.ps
    new_optstate = state.optstate
    if state.opt isa NNOptim.Optimizer
        pvec = collect_array_leaves(new_ps)
        gvec = collect_array_leaves(gs)
        update!(state.opt, pvec, gvec)
    else
        new_optstate, new_ps = Optimisers.update(state.optstate, state.ps, gs)
    end

    # Learning rate (best-effort; may be absent in some rules)
    lr = try getfield(state.opt, :η) catch; try getfield(state.opt, :eta) catch; try getfield(state.opt, :lr) catch; NaN end end end

